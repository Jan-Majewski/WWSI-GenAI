{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1d9cd1",
   "metadata": {},
   "outputs": [],
   "source": "import requests\nimport json\n\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate"
  },
  {
   "cell_type": "markdown",
   "id": "f893efd7",
   "metadata": {},
   "source": [
    "## Setup LM studio endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2bf1eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LM Studio endpoint\n",
    "url = \"http://127.0.0.1:1234/v1/chat/completions\"\n",
    "\n",
    "# Define your question\n",
    "question = \"What can you tell me about large language models?\"\n",
    "\n",
    "# Prepare the payload\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ],\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": 50\n",
    "}\n",
    "\n",
    "# Set headers for the API request\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc523fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n",
      "Okay, let's break down Large Language Models (LLMs). They're a *very* hot topic right now and driving a lot of innovation. Here's a comprehensive overview covering what they are, how they work, their capabilities\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Send the request\n",
    "try:\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "    \n",
    "    # Parse the response\n",
    "    result = response.json()\n",
    "    \n",
    "    # Extract and print the answer\n",
    "    if \"choices\" in result and len(result[\"choices\"]) > 0:\n",
    "        answer = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "        print(\"Model response:\")\n",
    "        print(answer)\n",
    "    else:\n",
    "        print(\"Unexpected response format:\", result)\n",
    "        \n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error making request: {e}\")\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error parsing response as JSON: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f3b907",
   "metadata": {},
   "source": [
    "## Local LLMs with Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cf7cef",
   "metadata": {},
   "outputs": [],
   "source": "# LM Studio provides an OpenAI-compatible API\n# We use ChatOpenAI from langchain_openai and point it to local endpoint\nllm = ChatOpenAI(\n    model=\"gemma-3\",  # This can be any string as LM Studio will use the loaded model\n    base_url=\"http://127.0.0.1:1234/v1\",  # LM Studio local endpoint\n    api_key=\"not-needed\",  # LM Studio doesn't require an API key but the parameter is needed\n    temperature=0.7,\n    max_tokens=500,\n    streaming=True\n)"
  },
  {
   "cell_type": "markdown",
   "id": "8a20d478",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39cd6ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n",
      "Okay, let's dive into Large Language Models (LLMs). They're a *very* hot topic right now, and for good reason – they're fundamentally changing how we interact with computers. Here's a breakdown covering what they are, how they work, their capabilities, limitations, and current trends.  I'll try to be as clear as possible without getting *too* technical (though some technical explanation is necessary).\n",
      "\n",
      "**1. What ARE Large Language Models?**\n",
      "\n",
      "* **Essentially, sophisticated text predictors:** At their core, LLMs are designed to predict the next word in a sequence of words. They do this based on patterns they've learned from massive amounts of text data.\n",
      "* **\"Large\" refers to two things:**\n",
      "    * **Size of Data:**  They're trained on *massive* datasets – think billions or even trillions of words scraped from the internet, books, articles, code repositories, and more.\n",
      "    * **Number of Parameters:** \"Parameters\" are essentially the adjustable knobs within the model that allow it to learn complex relationships in data. LLMs have billions (and increasingly *trillions*) of these parameters.  More parameters generally mean greater capacity for learning and potentially better performance.\n",
      "* **Examples:** Popular LLMs include:\n",
      "    * **GPT Series (OpenAI):** GPT-3, GPT-4, etc. – Known for their impressive text generation abilities.\n",
      "    * **LaMDA (Google):** Focused on conversational ability.\n",
      "    * **PaLM & Gemini (Google):**  Powerful models with multimodal capabilities (text *and* images/audio).\n",
      "    * **Llama Series (Meta):** Open-source LLMs, increasingly competitive.\n",
      "    * **Claude (Anthropic):** Designed for safety and helpfulness.\n",
      "\n",
      "**2. How Do They Work? (Simplified Explanation)**\n",
      "\n",
      "* **The Transformer Architecture:** Most modern LLMs are built on a \"transformer\" architecture.  This is a specific neural network design that's exceptionally good at understanding context in sequences of data, like text.\n",
      "    * **Attention Mechanism:** The key innovation of transformers is the \"attention mechanism.\" This allows the model to focus on different parts of the input sequence when making predictions. For example, if you ask \"The cat sat on the mat. What color was it?\", the attention mechanism helps the LLM realize that \"it\" refers\n"
     ]
    }
   ],
   "source": [
    "# Create a simple prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Create a chain that will pass the question to the model\n",
    "chain = prompt | llm\n",
    "\n",
    "# Execute the chain\n",
    "try:\n",
    "    result = chain.invoke({\"input\": question})\n",
    "    print(\"Model response:\")\n",
    "    print(result.content)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ec68e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arisa-genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}